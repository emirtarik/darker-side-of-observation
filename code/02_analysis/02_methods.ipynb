{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "9lz9nXzP91Xv"
      },
      "outputs": [],
      "source": [
        "#!pip install lazypredict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZWGzT6Oz5dK"
      },
      "source": [
        "# INFO Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "RV3fHCWF_br5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "config_path = os.path.join('..', 'config.json')\n",
        "\n",
        "with open(config_path, 'r') as config_file:\n",
        "    config = json.load(config_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c10SP37k7n81"
      },
      "source": [
        "### 1. Data - info only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "XxqCGs1r-mnE"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = None\n",
        "data_d = None\n",
        "data_t2 = None\n",
        "\n",
        "X = None\n",
        "Y = None\n",
        "\n",
        "X_train = None\n",
        "X_test = None\n",
        "y_train = None\n",
        "y_test = None\n",
        "\n",
        "data_path = config['data_path']\n",
        "\n",
        "input_path1 = os.path.join(data_path, 'analysis_to_python.dta')\n",
        "data = pd.read_stata(input_path1, convert_categoricals=False)\n",
        "\n",
        "# data subset\n",
        "data_t2 = data[(data[\"treatment\"]==6) & (data[\"info\"]==1)] # train-test taken from treatment 6 and info only\n",
        "# data_t2 = data_t2[[\"donation\", \"age\", \"educ\", \"income\", \"female\", \"children\", \"prev_donation_charity\", \"prev_donation_foreigner\", \"prev_volunteering\", \"prev_blooddonation\"]]\n",
        "data_t2 = data_t2[[\"donation\", \"age\", \"educ\", \"income\", \"female\", \"children\"]]\n",
        "\n",
        "data_t2 = data_t2.dropna() # 2 observations dropped\n",
        "\n",
        "# X = data_t2[[\"age\", \"educ\", \"income\", \"female\", \"children\", \"prev_donation_charity\", \"prev_donation_foreigner\", \"prev_volunteering\", \"prev_blooddonation\"]]\n",
        "# y = data_t2[\"donation\"]\n",
        "\n",
        "X = data_t2[[\"age\", \"educ\", \"income\", \"female\", \"children\"]]\n",
        "y = data_t2[\"donation\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y,  test_size=0.2, random_state = 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liw6ZfMI7vaj"
      },
      "source": [
        "### 1.1 LazyClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "B0Q6QyRI-mkY"
      },
      "outputs": [],
      "source": [
        "# import lazypredict\n",
        "# from lazypredict.Supervised import LazyClassifier\n",
        "\n",
        "# clf = LazyClassifier(verbose=0,ignore_warnings=True)\n",
        "# models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
        "\n",
        "# print(models.to_markdown())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc8IHElRzNrE"
      },
      "source": [
        "## 2. ML Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "i1gV_urDJWCE"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRx-gOjF7hqQ"
      },
      "source": [
        "### 2.1 Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "3T2QkOzU1i1J"
      },
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# # RF with 3 features and 100 trees\n",
        "# rf3 = RandomForestClassifier(n_estimators=100, max_features=3, random_state=334)\n",
        "# rfOpt = rf3.fit(X_train, y_train)\n",
        "\n",
        "# # Prediction on the test set\n",
        "\n",
        "# yChapRF = rfOpt.predict(X_test)\n",
        "\n",
        "# # Confusion matrix\n",
        "\n",
        "# tableRF = pd.crosstab(yChapRF, y_test)\n",
        "# print(tableRF)\n",
        "\n",
        "# # Prediction error on the test set\n",
        "# print(\"Test accuracy - Random forest = %f\" % (rfOpt.score(X_test, y_test)))\n",
        "\n",
        "# ### Accuracy with random forest opt -quantitative = 0.798450"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "zoXXHg9E-gbD"
      },
      "outputs": [],
      "source": [
        "# importances_total = rf3.feature_importances_\n",
        "# std_total = np.std([rf3.feature_importances_ for tree in rf3.estimators_], axis=0)\n",
        "\n",
        "# feature_names_total = X.columns.values.tolist()\n",
        "\n",
        "# forest_importances_total = pd.Series(importances_total, index=feature_names_total)\n",
        "\n",
        "# plt.figure(figsize=(6,4))\n",
        "# fig, ax = plt.subplots()\n",
        "# forest_importances_total.plot.bar(yerr=std_total, ax=ax)\n",
        "# ax.set_title(\"RF - Feature importances using MDI\")\n",
        "# ax.set_ylabel(\"Mean decrease in impurity\")\n",
        "# fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "jyr_fkwBAmo_"
      },
      "outputs": [],
      "source": [
        "# %matplotlib inline\n",
        "\n",
        "# (pd.Series(importances_total, index=feature_names_total)\n",
        "#    .nlargest(20)\n",
        "#    .plot(kind='barh'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgMtJ_wbZybw"
      },
      "source": [
        "Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Usdz-U7LB1dA"
      },
      "outputs": [],
      "source": [
        "# from scipy.stats import randint\n",
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# rs_space={'max_depth':list(np.arange(10, 100, step=10)) + [None],\n",
        "#               'n_estimators':np.arange(10, 500, step=50),\n",
        "#               'max_features':randint(1,7),\n",
        "#               'criterion':['gini','entropy'],\n",
        "#               'min_samples_leaf':randint(1,4),\n",
        "#               'min_samples_split':np.arange(2, 10, step=2)\n",
        "#          }\n",
        "\n",
        "# rf = RandomForestClassifier()\n",
        "# rf_random = RandomizedSearchCV(rf, rs_space, n_iter=500, scoring='accuracy', n_jobs=-1, cv=3)\n",
        "# model_random = rf_random.fit(X,y)\n",
        "\n",
        "\n",
        "# print('Best hyperparameters are: '+str(model_random.best_params_))\n",
        "# print('Best score is: '+str(model_random.best_score_))\n",
        "\n",
        "# # Best hyperparameters are: {'criterion': 'gini', 'max_depth': 40, 'max_features': 1, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 60}\n",
        "# # Best score is: 0.7477612726506532"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "0K7SfU7GFRC7"
      },
      "outputs": [],
      "source": [
        "# # RF with tuned parameters\n",
        "# rf_opt = RandomForestClassifier(criterion='gini', max_depth=40, min_samples_leaf=3, min_samples_split=2, n_estimators=60, max_features=1, random_state=222)\n",
        "# rf_opt_fit = rf_opt.fit(X_train, y_train)\n",
        "\n",
        "# # Prediction on the test set\n",
        "# yChapRF = None\n",
        "# yChapRF = rf_opt_fit.predict(X_test)\n",
        "\n",
        "# # Confusion matrix\n",
        "# tableRF = None\n",
        "# tableRF = pd.crosstab(yChapRF, y_test)\n",
        "# print(tableRF)\n",
        "\n",
        "# # Prediction error on the test set\n",
        "# print(\"Test accuracy - Random forest opt= %f\" % (rfOpt.score(X_test, y_test)))\n",
        "\n",
        "# ### Accuracy with random forest opt = 0.764706"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "GSlDehq_df7U"
      },
      "outputs": [],
      "source": [
        "# print(tableRF.to_latex(index=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "65VKHcIOJCJ-"
      },
      "outputs": [],
      "source": [
        "# # Predict Passive observer donations\n",
        "# data_t4 = None\n",
        "# data_t4_sub = None\n",
        "# data_m = None\n",
        "\n",
        "# data_t4 = data[data[\"treatment\"]==4]\n",
        "# data_t4_sub = data_t4[[\"age\", \"educ\", \"income\", \"female\", \"children\", \"prev_donation_charity\", \"prev_donation_foreigner\", \"prev_volunteering\", \"prev_blooddonation\"]]\n",
        "# data_t4_sub = data_t4_sub.fillna(data_t4_sub.mean()) # fill NAs with average (two rows)\n",
        "\n",
        "# t4_pred = rf_opt_fit.predict(data_t4_sub)\n",
        "\n",
        "# data_t4['rfpred_donation'] = t4_pred\n",
        "# data_m = pd.merge(data, data_t4[['ID', 'rfpred_donation']], on='ID', how='left')\n",
        "\n",
        "# data_m.to_stata('data_predicted.dta', version=118)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7BKbZu1FMG7"
      },
      "source": [
        "### 2.2 Gaussian NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpK3vgYNZtNo",
        "outputId": "438ff4f2-143c-4d67-c14e-5fdfdda359cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   row_0 |   0.0 |   1.0 |\n",
            "|--------:|------:|------:|\n",
            "|       0 |    37 |    11 |\n",
            "|       1 |    12 |    11 |\n",
            "\\begin{tabular}{lrr}\n",
            "\\toprule\n",
            "donation &  0.0 &  1.0 \\\\\n",
            "row\\_0 &      &      \\\\\n",
            "\\midrule\n",
            "0     &   37 &   11 \\\\\n",
            "1     &   12 &   11 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/3853584826.py:21: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
            "  print(tableG.to_latex(index=True))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "GNB = None\n",
        "\n",
        "yChapL = None\n",
        "table = None\n",
        "\n",
        "\n",
        "GNB = GaussianNB().fit(X_train, y_train)\n",
        "\n",
        "# custom threshold for binary predictions\n",
        "def custom_predictG(X, threshold):\n",
        "    probs = GNB.predict_proba(X)\n",
        "    return (probs[:, 1] > threshold).astype(int)\n",
        "\n",
        "# Prediction\n",
        "yChapG = custom_predictG(X_test, 0.42)\n",
        "\n",
        "tableG = pd.crosstab(yChapG, y_test)\n",
        "print(tableG.to_markdown())\n",
        "print(tableG.to_latex(index=True))\n",
        "\n",
        "### Comment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIsSOcCrdeyJ"
      },
      "source": [
        "### 2.3 K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ7cYBi9dfCF",
        "outputId": "9c0d41d5-f44b-4c58-b531-76b6906b33de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best learning rate = 0.263910, Best parameter  = {'n_neighbors': 8}\n",
            "|   row_0 |   0.0 |   1.0 |\n",
            "|--------:|------:|------:|\n",
            "|       0 |    33 |    11 |\n",
            "|       1 |    16 |    11 |\n",
            "Test accuracy - Logistic regression  Lasso = 0.661972\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "KNN = None\n",
        "KNN_CV = None\n",
        "KNN_opt = None\n",
        "KNN_opt_fit = None\n",
        "\n",
        "yChapK = None\n",
        "table = None\n",
        "\n",
        "# Grid of penalty parameters\n",
        "param = None\n",
        "param = [{\"n_neighbors\": [1,2,3,4,5,6,7,8,9,10]}] # dictionary of the values of n_neighbors used for GridSearchCV\n",
        "\n",
        "\n",
        "KNN_CV = GridSearchCV(KNeighborsClassifier(), param ,cv =5, n_jobs = -1)\n",
        "KNN = KNN_CV.fit(X_train, y_train)\n",
        "\n",
        "KNN.best_params_[\"n_neighbors\"] # best n_neighbor is 8\n",
        "\n",
        "# Computation of the loss and determining best parameter\n",
        "\n",
        "print(\"Best learning rate = %f, Best parameter  = %s\" %\n",
        "      (1.-KNN.best_score_ , KNN.best_params_))\n",
        "\n",
        "# Optimized model using the best parameter\n",
        "KNN_opt = KNeighborsClassifier(n_neighbors=8)\n",
        "KNN_opt_fit = KNN_opt.fit(X_train, y_train)\n",
        "\n",
        "# custom threshold for binary predictions\n",
        "def custom_predict(X, threshold):\n",
        "    probs = KNN_opt.predict_proba(X)\n",
        "    return (probs[:, 1] > threshold).astype(int)\n",
        "\n",
        "# Prediction\n",
        "yChapK = custom_predict(X_test, 0.37499)\n",
        "\n",
        "table = pd.crosstab(yChapK, y_test)\n",
        "print(table.to_markdown())\n",
        "\n",
        "# Error on the test set\n",
        "print(\"Test accuracy - Logistic regression  Lasso = %f\" % (KNN_opt_fit.score(X_test, y_test)))\n",
        "r2_score_KNN = KNN_opt_fit.score(X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Comment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqiLZn96VXQq"
      },
      "source": [
        "### 2.4 Linear Probability **Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtM1ZVsNJUo7",
        "outputId": "83225b79-6f33-482e-c0ef-34bb0cb5e2a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   row_0 |   0.0 |   1.0 |\n",
            "|--------:|------:|------:|\n",
            "|       0 |    37 |    12 |\n",
            "|       1 |    12 |    10 |\n",
            "\\begin{tabular}{lrr}\n",
            "\\toprule\n",
            "donation &  0.0 &  1.0 \\\\\n",
            "row\\_0 &      &      \\\\\n",
            "\\midrule\n",
            "0     &   37 &   12 \\\\\n",
            "1     &   12 &   10 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/2572589795.py:19: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
            "  print(tableLPM.to_latex(index=True))\n"
          ]
        }
      ],
      "source": [
        "linearPM_fit = None\n",
        "\n",
        "yChapLPM = None\n",
        "table = None\n",
        "\n",
        "\n",
        "linearPM_fit = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "# custom threshold for binary predictions\n",
        "def custom_predictLPM(X, threshold):\n",
        "    probs = linearPM_fit.predict(X)\n",
        "    return (probs > threshold).astype(int)\n",
        "\n",
        "# Prediction\n",
        "yChapLPM = custom_predictLPM(X_test, 0.325)\n",
        "\n",
        "tableLPM = pd.crosstab(yChapLPM, y_test)\n",
        "print(tableLPM.to_markdown())\n",
        "print(tableLPM.to_latex(index=True))\n",
        "\n",
        "\n",
        "### Comment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4SXcsbCJBCJ"
      },
      "source": [
        "### 2.5 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "647oAg2SJG33"
      },
      "source": [
        "#### 2.5.1 Lasso optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6Ztf6bZVW1b",
        "outputId": "d7e2c421-8f4f-4030-8cff-1679ed703b6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best learning rate = 0.257018, Best parameter  = {'C': 0.001}\n",
            "|   row_0 |   0.0 |   1.0 |\n",
            "|--------:|------:|------:|\n",
            "|       0 |    33 |    16 |\n",
            "|       1 |    16 |     6 |\n",
            "Test accuracy - Logistic regression  Lasso = 0.690141\n",
            "\\begin{tabular}{lrr}\n",
            "\\toprule\n",
            "donation &  0.0 &  1.0 \\\\\n",
            "row\\_0 &      &      \\\\\n",
            "\\midrule\n",
            "0     &   33 &   16 \\\\\n",
            "1     &   16 &    6 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/44237097.py:43: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
            "  print(tableLL.to_latex(index=True))\n"
          ]
        }
      ],
      "source": [
        "logitL = None\n",
        "logitLasso = None\n",
        "logitL_opt = None\n",
        "logitL_opt_fit = None\n",
        "yChapL = None\n",
        "table = None\n",
        "\n",
        "# Grid of penalty parameters\n",
        "param = None\n",
        "param = [{\"C\": [0.001,0.01,0.1,0.5,0.75,0.9,1,1.5,2,2.5,5]}] # dictionary of the values of C used for GridSearchCV\n",
        "\n",
        "#COMMENT: We use the following codes to get logistic regression + l1 penalty (LASSO case)\n",
        "\n",
        "logitL = GridSearchCV(LogisticRegression(penalty=\"l1\", solver=\"liblinear\"), param ,cv =5, n_jobs = -1)\n",
        "logitLasso = logitL.fit(X_train, y_train)\n",
        "\n",
        "logitLasso.best_params_[\"C\"]\n",
        "\n",
        "# Computation of the loss and determining best parameter\n",
        "\n",
        "print(\"Best learning rate = %f, Best parameter  = %s\" %\n",
        "      (1.-logitLasso.best_score_ , logitLasso.best_params_))\n",
        "\n",
        "# Optimized model using the best parameter\n",
        "logitL_opt = LogisticRegression(C=0.001, penalty=\"l1\", solver=\"liblinear\")\n",
        "logitL_opt_fit = logitL_opt.fit(X_train, y_train)\n",
        "\n",
        "# custom threshold for binary predictions\n",
        "def custom_predictL(X, threshold):\n",
        "    probs = logitL_opt_fit.predict_proba(X)\n",
        "    return (probs[:, 1] > threshold).astype(int)\n",
        "\n",
        "# Prediction\n",
        "yChapL = custom_predictL(X_test, 0.36)\n",
        "\n",
        "tableLL = pd.crosstab(yChapL, y_test)\n",
        "print(tableLL.to_markdown())\n",
        "\n",
        "# Error on the test set\n",
        "print(\"Test accuracy - Logistic regression  Lasso = %f\" % (logitL_opt_fit.score(X_test, y_test)))\n",
        "r2_score_lasso = logitL_opt_fit.score(X_test, y_test)\n",
        "\n",
        "print(tableLL.to_latex(index=True))\n",
        "\n",
        "\n",
        "### Comment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRGG2arzJPRc"
      },
      "source": [
        "#### 2.5.2 Ridge optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm5XR1krJT2D",
        "outputId": "c9f560c7-8d62-4bd7-95cd-39e97a43f1d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best score = 0.257018, Best parameter = {'C': 0.001}\n",
            "|   row_0 |   0.0 |   1.0 |\n",
            "|--------:|------:|------:|\n",
            "|       0 |    37 |    12 |\n",
            "|       1 |    12 |    10 |\n",
            "Test accuracy - Logistic regression Ridge = 0.690141\n",
            "\\begin{tabular}{lrr}\n",
            "\\toprule\n",
            "donation &  0.0 &  1.0 \\\\\n",
            "row\\_0 &      &      \\\\\n",
            "\\midrule\n",
            "0     &   37 &   12 \\\\\n",
            "1     &   12 &   10 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/3049714600.py:41: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
            "  print(tableLR.to_latex(index=True))\n"
          ]
        }
      ],
      "source": [
        "logitR = None\n",
        "logitRidge = None\n",
        "logitR_opt = None\n",
        "logitR_opt_fit = None\n",
        "yChapR = None\n",
        "tableR = None\n",
        "\n",
        "# Grid of penalty parameters\n",
        "param = None\n",
        "param = [{\"C\":[0.001,0.005,0.01,0.1,0.25,0.5,0.75,1,2.5,5,7,10,15,20]}]\n",
        "\n",
        "\n",
        "logitR = GridSearchCV(LogisticRegression(penalty=\"l2\", solver=\"liblinear\"), param, cv = 5, n_jobs = -1)\n",
        "logitRidge = logitR.fit(X_train, y_train)\n",
        "logitRidge.best_params_[\"C\"]\n",
        "\n",
        "# Error computation\n",
        "print(\"Best score = %f, Best parameter = %s\" %\n",
        "      (1. - logitRidge.best_score_, logitRidge.best_params_))\n",
        "\n",
        "# Optimized model using the best parameter\n",
        "logitR_opt = LogisticRegression(C=15, penalty=\"l2\", solver=\"liblinear\")\n",
        "logitR_opt_fit = logitR_opt.fit(X_train, y_train)\n",
        "\n",
        "# custom threshold for binary predictions\n",
        "def custom_predictR(X, threshold):\n",
        "    probs = logitR_opt_fit.predict_proba(X)\n",
        "    return (probs[:, 1] > threshold).astype(int)\n",
        "\n",
        "# Prediction\n",
        "yChapR = custom_predictR(X_test, 0.32)\n",
        "\n",
        "# confusion matrix\n",
        "tableLR = pd.crosstab(yChapR, y_test)\n",
        "print(tableLR.to_markdown())\n",
        "\n",
        "# Error on the test set\n",
        "print(\"Test accuracy - Logistic regression Ridge = %f\" % (logitR_opt_fit.score(X_test, y_test)))\n",
        "r2_score_ridge = logitR_opt_fit.score(X_test, y_test)\n",
        "\n",
        "print(tableLR.to_latex(index=True))\n",
        "\n",
        "### Comment: slightly better than Lasso, in all runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Zbok7MzHjk"
      },
      "source": [
        "## 3. Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OdKtT4dR8jH",
        "outputId": "3be65a6f-62d5-44a0-80d4-942680d421b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/2398330045.py:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_info['LLpred_donation'] = info_Lpred\n",
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/2398330045.py:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_info['LRpred_donation'] = info_Rpred\n",
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/2398330045.py:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_info['LPMpred_donation'] = info_LPMpred\n",
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/2398330045.py:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_info['Gpred_donation'] = info_Gpred\n",
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/2398330045.py:37: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_both['LLpred_donation_allwithinfo'] = both_Lpred\n",
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/2398330045.py:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_both['LRpred_donation_allwithinfo'] = both_Rpred\n",
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/2398330045.py:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_both['LPMpred_donation_allwithinfo'] = both_LPMpred\n",
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/2398330045.py:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_both['Gpred_donation_allwithinfo'] = both_Gpred\n"
          ]
        }
      ],
      "source": [
        "# Predict Passive observer donations\n",
        "data_t46_info = None\n",
        "data_t46_info_sub = None\n",
        "data_m = None\n",
        "data_m0 = None\n",
        "\n",
        "data_t46_both = None\n",
        "data_t46_both_sub = None\n",
        "\n",
        "# subset with only info to predict on\n",
        "data_t46_info = data[((data[\"treatment\"]==4) | (data[\"treatment\"]==6)) & (data[\"info\"]==1)]\n",
        "data_t46_info_sub = data_t46_info[[\"age\", \"educ\", \"income\", \"female\", \"children\"]]\n",
        "data_t46_info_sub = data_t46_info_sub.fillna(data_t46_info_sub.mean()) # fill NAs with average\n",
        "\n",
        "# subset with both to predict on\n",
        "data_t46_both = data[(data[\"treatment\"]==4) | (data[\"treatment\"]==6)]\n",
        "data_t46_both_sub = data_t46_both[[\"age\", \"educ\", \"income\", \"female\", \"children\"]]\n",
        "data_t46_both_sub = data_t46_both_sub.fillna(data_t46_both_sub.mean()) # fill NAs with average\n",
        "\n",
        "# prediction\n",
        "info_Lpred = custom_predictL(data_t46_info_sub, 0.36)\n",
        "info_Rpred = custom_predictR(data_t46_info_sub, 0.32)\n",
        "info_LPMpred = custom_predictLPM(data_t46_info_sub, 0.325)\n",
        "info_Gpred = custom_predictG(data_t46_info_sub, 0.42)\n",
        "\n",
        "both_Lpred = custom_predictL(data_t46_both_sub, 0.36)\n",
        "both_Rpred = custom_predictR(data_t46_both_sub, 0.32)\n",
        "both_LPMpred = custom_predictLPM(data_t46_both_sub, 0.325)\n",
        "both_Gpred = custom_predictG(data_t46_both_sub, 0.42)\n",
        "\n",
        "\n",
        "data_t46_info['LLpred_donation'] = info_Lpred\n",
        "data_t46_info['LRpred_donation'] = info_Rpred\n",
        "data_t46_info['LPMpred_donation'] = info_LPMpred\n",
        "data_t46_info['Gpred_donation'] = info_Gpred\n",
        "\n",
        "data_t46_both['LLpred_donation_allwithinfo'] = both_Lpred\n",
        "data_t46_both['LRpred_donation_allwithinfo'] = both_Rpred\n",
        "data_t46_both['LPMpred_donation_allwithinfo'] = both_LPMpred\n",
        "data_t46_both['Gpred_donation_allwithinfo'] = both_Gpred\n",
        "\n",
        "# merging\n",
        "data_m0 = pd.merge(data_t46_both, data_t46_info[['ID', 'LLpred_donation', 'LRpred_donation', 'LPMpred_donation', 'Gpred_donation']], on='ID', how='left') # first merging no-info with all-subset\n",
        "data_m = pd.merge(data, data_m0[['ID', 'LLpred_donation', 'LRpred_donation', 'LPMpred_donation', 'Gpred_donation', 'LLpred_donation_allwithinfo', 'LRpred_donation_allwithinfo', 'LPMpred_donation_allwithinfo', 'Gpred_donation_allwithinfo']], on='ID', how='left') # second merging all-subset with whole data\n",
        "\n",
        "output_path1 = os.path.join(data_path, 'data_predicted_info.dta')\n",
        "data_m.to_stata(output_path1, version=118)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "oO1BNoEm4zQJ"
      },
      "outputs": [],
      "source": [
        "%reset -f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc6x6kSV9EhR"
      },
      "source": [
        "# NO INFO Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "aZWS0vta-VMA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "config_path = os.path.join('..', 'config.json')\n",
        "\n",
        "with open(config_path, 'r') as config_file:\n",
        "    config = json.load(config_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eW9QX0V9YGI"
      },
      "source": [
        "### 1. Data - no info only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ywOIOmBj9YGU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = None\n",
        "data_d = None\n",
        "data_t2 = None\n",
        "\n",
        "X = None\n",
        "Y = None\n",
        "\n",
        "X_train = None\n",
        "X_test = None\n",
        "y_train = None\n",
        "y_test = None\n",
        "\n",
        "data_path = config['data_path']\n",
        "\n",
        "input_path1 = os.path.join(data_path, 'analysis_to_python.dta')\n",
        "data = pd.read_stata(input_path1, convert_categoricals=False)\n",
        "\n",
        "\n",
        "# data subset\n",
        "data_t2 = data[(data[\"treatment\"]==6) & (data[\"info\"]==0)] # train-test taken from treatment 6 and no-info only\n",
        "# data_t2 = data_t2[[\"donation\", \"age\", \"educ\", \"income\", \"female\", \"children\", \"prev_donation_charity\", \"prev_donation_foreigner\", \"prev_volunteering\", \"prev_blooddonation\"]]\n",
        "data_t2 = data_t2[[\"donation\", \"age\", \"educ\", \"income\", \"female\", \"children\"]]\n",
        "\n",
        "data_t2 = data_t2.dropna() # 2 observations dropped\n",
        "\n",
        "# X = data_t2[[\"age\", \"educ\", \"income\", \"female\", \"children\", \"prev_donation_charity\", \"prev_donation_foreigner\", \"prev_volunteering\", \"prev_blooddonation\"]]\n",
        "# y = data_t2[\"donation\"]\n",
        "\n",
        "X = data_t2[[\"age\", \"educ\", \"income\", \"female\", \"children\"]]\n",
        "y = data_t2[\"donation\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y,  test_size=0.2, random_state = 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF5oVijJ9YGV"
      },
      "source": [
        "## 2. ML Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "kaftAXqF9YGV"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wf-6P1U9YGV"
      },
      "source": [
        "### 2.1 Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "HjIpMzrx9YGV"
      },
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# # RF with 3 features and 100 trees\n",
        "# rf3 = RandomForestClassifier(n_estimators=100, max_features=3, random_state=334)\n",
        "# rfOpt = rf3.fit(X_train, y_train)\n",
        "\n",
        "# # Prediction on the test set\n",
        "\n",
        "# yChapRF = rfOpt.predict(X_test)\n",
        "\n",
        "# # Confusion matrix\n",
        "\n",
        "# tableRF = pd.crosstab(yChapRF, y_test)\n",
        "# print(tableRF)\n",
        "\n",
        "# # Prediction error on the test set\n",
        "# print(\"Test accuracy - Random forest = %f\" % (rfOpt.score(X_test, y_test)))\n",
        "\n",
        "# ### Accuracy with random forest opt -quantitative = 0.798450"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "2C6zNSqt9YGW"
      },
      "outputs": [],
      "source": [
        "# importances_total = rf3.feature_importances_\n",
        "# std_total = np.std([rf3.feature_importances_ for tree in rf3.estimators_], axis=0)\n",
        "\n",
        "# feature_names_total = X.columns.values.tolist()\n",
        "\n",
        "# forest_importances_total = pd.Series(importances_total, index=feature_names_total)\n",
        "\n",
        "# plt.figure(figsize=(6,4))\n",
        "# fig, ax = plt.subplots()\n",
        "# forest_importances_total.plot.bar(yerr=std_total, ax=ax)\n",
        "# ax.set_title(\"RF - Feature importances using MDI\")\n",
        "# ax.set_ylabel(\"Mean decrease in impurity\")\n",
        "# fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "tlKxEyFJ9YGW"
      },
      "outputs": [],
      "source": [
        "# %matplotlib inline\n",
        "\n",
        "# (pd.Series(importances_total, index=feature_names_total)\n",
        "#    .nlargest(20)\n",
        "#    .plot(kind='barh'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42M9myWm9YGW"
      },
      "source": [
        "Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "ZuyUh0kt9YGW"
      },
      "outputs": [],
      "source": [
        "# from scipy.stats import randint\n",
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# rs_space={'max_depth':list(np.arange(10, 100, step=10)) + [None],\n",
        "#               'n_estimators':np.arange(10, 500, step=50),\n",
        "#               'max_features':randint(1,7),\n",
        "#               'criterion':['gini','entropy'],\n",
        "#               'min_samples_leaf':randint(1,4),\n",
        "#               'min_samples_split':np.arange(2, 10, step=2)\n",
        "#          }\n",
        "\n",
        "# rf = RandomForestClassifier()\n",
        "# rf_random = RandomizedSearchCV(rf, rs_space, n_iter=500, scoring='accuracy', n_jobs=-1, cv=3)\n",
        "# model_random = rf_random.fit(X,y)\n",
        "\n",
        "\n",
        "# print('Best hyperparameters are: '+str(model_random.best_params_))\n",
        "# print('Best score is: '+str(model_random.best_score_))\n",
        "\n",
        "# # Best hyperparameters are: {'criterion': 'gini', 'max_depth': 40, 'max_features': 1, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 60}\n",
        "# # Best score is: 0.7477612726506532"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "XKCP5JHX9YGX"
      },
      "outputs": [],
      "source": [
        "# # RF with tuned parameters\n",
        "# rf_opt = RandomForestClassifier(criterion='gini', max_depth=40, min_samples_leaf=3, min_samples_split=2, n_estimators=60, max_features=1, random_state=222)\n",
        "# rf_opt_fit = rf_opt.fit(X_train, y_train)\n",
        "\n",
        "# # Prediction on the test set\n",
        "# yChapRF = None\n",
        "# yChapRF = rf_opt_fit.predict(X_test)\n",
        "\n",
        "# # Confusion matrix\n",
        "# tableRF = None\n",
        "# tableRF = pd.crosstab(yChapRF, y_test)\n",
        "# print(tableRF)\n",
        "\n",
        "# # Prediction error on the test set\n",
        "# print(\"Test accuracy - Random forest opt= %f\" % (rfOpt.score(X_test, y_test)))\n",
        "\n",
        "# ### Accuracy with random forest opt = 0.764706"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "NDrfv-ja9YGX"
      },
      "outputs": [],
      "source": [
        "# print(tableRF.to_latex(index=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "tAnpeRRM9YGX"
      },
      "outputs": [],
      "source": [
        "# # Predict Passive observer donations\n",
        "# data_t4 = None\n",
        "# data_t4_sub = None\n",
        "# data_m = None\n",
        "\n",
        "# data_t4 = data[data[\"treatment\"]==4]\n",
        "# data_t4_sub = data_t4[[\"age\", \"educ\", \"income\", \"female\", \"children\", \"prev_donation_charity\", \"prev_donation_foreigner\", \"prev_volunteering\", \"prev_blooddonation\"]]\n",
        "# data_t4_sub = data_t4_sub.fillna(data_t4_sub.mean()) # fill NAs with average (two rows)\n",
        "\n",
        "# t4_pred = rf_opt_fit.predict(data_t4_sub)\n",
        "\n",
        "# data_t4['rfpred_donation'] = t4_pred\n",
        "# data_m = pd.merge(data, data_t4[['ID', 'rfpred_donation']], on='ID', how='left')\n",
        "\n",
        "# data_m.to_stata('data_predicted.dta', version=118)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRIP4t3p9YGY"
      },
      "source": [
        "### 2.2 Gaussian NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF_6dY-c-nsr",
        "outputId": "4e2274c7-b30b-4ab9-eff1-22fba67e9096"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   donation  count\n",
            "0       0.0     41\n",
            "1       1.0     19\n"
          ]
        }
      ],
      "source": [
        "# prompt: create a table showing the frequency of 'donation' values in 'X_test'\n",
        "\n",
        "donation_counts = y_test.value_counts()\n",
        "print(donation_counts.to_frame().reset_index().rename(columns={'index':'donation', 'donation':'count'}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGlmSIMa9YGY",
        "outputId": "33743868-7130-4280-f403-774013f94de7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   row_0 |   0.0 |   1.0 |\n",
            "|--------:|------:|------:|\n",
            "|       0 |    32 |    13 |\n",
            "|       1 |     9 |     6 |\n",
            "\\begin{tabular}{lrr}\n",
            "\\toprule\n",
            "donation &  0.0 &  1.0 \\\\\n",
            "row\\_0 &      &      \\\\\n",
            "\\midrule\n",
            "0     &   32 &   13 \\\\\n",
            "1     &    9 &    6 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/546946962.py:21: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
            "  print(tableG.to_latex(index=True))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "GNB = None\n",
        "\n",
        "yChapL = None\n",
        "table = None\n",
        "\n",
        "\n",
        "GNB = GaussianNB().fit(X_train, y_train)\n",
        "\n",
        "# custom threshold for binary predictions\n",
        "def custom_predictG(X, threshold):\n",
        "    probs = GNB.predict_proba(X)\n",
        "    return (probs[:, 1] > threshold).astype(int)\n",
        "\n",
        "# Prediction\n",
        "yChapG = custom_predictG(X_test, 0.2386)\n",
        "\n",
        "tableG = pd.crosstab(yChapG, y_test)\n",
        "print(tableG.to_markdown())\n",
        "print(tableG.to_latex(index=True))\n",
        "\n",
        "### Comment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxjlS5fE9YGY"
      },
      "source": [
        "### 2.3 K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8WMjI_N9YGZ",
        "outputId": "bb781fde-8918-435e-e681-5c99fe034078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best learning rate = 0.179876, Best parameter  = {'n_neighbors': 4}\n",
            "|   row_0 |   0.0 |   1.0 |\n",
            "|--------:|------:|------:|\n",
            "|       0 |    35 |    12 |\n",
            "|       1 |     6 |     7 |\n",
            "Test accuracy - Logistic regression  Lasso = 0.683333\n",
            "\\begin{tabular}{lrr}\n",
            "\\toprule\n",
            "donation &  0.0 &  1.0 \\\\\n",
            "row\\_0 &      &      \\\\\n",
            "\\midrule\n",
            "0     &   35 &   12 \\\\\n",
            "1     &    6 &    7 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/3898371262.py:46: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
            "  print(tableKNN.to_latex(index=True))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "KNN = None\n",
        "KNN_CV = None\n",
        "KNN_opt = None\n",
        "KNN_opt_fit = None\n",
        "\n",
        "yChapK = None\n",
        "table = None\n",
        "\n",
        "# Grid of penalty parameters\n",
        "param = None\n",
        "param = [{\"n_neighbors\": [1,2,3,4,5,6,7,8,9,10]}] # dictionary of the values of n_neighbors used for GridSearchCV\n",
        "\n",
        "\n",
        "KNN_CV = GridSearchCV(KNeighborsClassifier(), param ,cv =5, n_jobs = -1)\n",
        "KNN = KNN_CV.fit(X_train, y_train)\n",
        "\n",
        "KNN.best_params_[\"n_neighbors\"] # best n_neighbor is 8\n",
        "\n",
        "# Computation of the loss and determining best parameter\n",
        "\n",
        "print(\"Best learning rate = %f, Best parameter  = %s\" %\n",
        "      (1.-KNN.best_score_ , KNN.best_params_))\n",
        "\n",
        "# Optimized model using the best parameter\n",
        "KNN_opt = KNeighborsClassifier(n_neighbors=8)\n",
        "KNN_opt_fit = KNN_opt.fit(X_train, y_train)\n",
        "\n",
        "# custom threshold for binary predictions\n",
        "def custom_predict(X, threshold):\n",
        "    probs = KNN_opt.predict_proba(X)\n",
        "    return (probs[:, 1] > threshold).astype(int)\n",
        "\n",
        "# Prediction\n",
        "yChapK = custom_predict(X_test, 0.37499)\n",
        "\n",
        "tableKNN = pd.crosstab(yChapK, y_test)\n",
        "print(tableKNN.to_markdown())\n",
        "\n",
        "# Error on the test set\n",
        "print(\"Test accuracy - Logistic regression  Lasso = %f\" % (KNN_opt_fit.score(X_test, y_test)))\n",
        "r2_score_KNN = KNN_opt_fit.score(X_test, y_test)\n",
        "\n",
        "\n",
        "print(tableKNN.to_latex(index=True))\n",
        "\n",
        "\n",
        "### Comment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYVgCp4i9YGZ"
      },
      "source": [
        "### 2.4 Linear Probability **Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJUFqSVE9YGZ",
        "outputId": "ce41610d-268e-48e0-c9c3-cf4b93109a88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   row_0 |   0.0 |   1.0 |\n",
            "|--------:|------:|------:|\n",
            "|       0 |    35 |    10 |\n",
            "|       1 |     6 |     9 |\n",
            "\\begin{tabular}{lrr}\n",
            "\\toprule\n",
            "donation &  0.0 &  1.0 \\\\\n",
            "row\\_0 &      &      \\\\\n",
            "\\midrule\n",
            "0     &   35 &   10 \\\\\n",
            "1     &    6 &    9 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/2325475714.py:19: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
            "  print(tableLPM.to_latex(index=True))\n"
          ]
        }
      ],
      "source": [
        "linearPM_fit = None\n",
        "\n",
        "yChapLPM = None\n",
        "table = None\n",
        "\n",
        "\n",
        "linearPM_fit = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "# custom threshold for binary predictions\n",
        "def custom_predictLPM(X, threshold):\n",
        "    probs = linearPM_fit.predict(X)\n",
        "    return (probs > threshold).astype(int)\n",
        "\n",
        "# Prediction\n",
        "yChapLPM = custom_predictLPM(X_test, 0.2455)\n",
        "\n",
        "tableLPM = pd.crosstab(yChapLPM, y_test)\n",
        "print(tableLPM.to_markdown())\n",
        "print(tableLPM.to_latex(index=True))\n",
        "\n",
        "\n",
        "### Comment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGdiKw8H9YGa"
      },
      "source": [
        "### 2.5 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smTzeoTT9YGa"
      },
      "source": [
        "#### 2.5.1 Lasso optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T2Wt8bc9YGa",
        "outputId": "9c41941b-c33c-4d37-c54f-a35e71cb87af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best learning rate = 0.179876, Best parameter  = {'C': 0.9}\n",
            "|   row_0 |   0.0 |   1.0 |\n",
            "|--------:|------:|------:|\n",
            "|       0 |    38 |    17 |\n",
            "|       1 |     3 |     2 |\n",
            "Test accuracy - Logistic regression  Lasso = 0.683333\n",
            "\\begin{tabular}{lrr}\n",
            "\\toprule\n",
            "donation &  0.0 &  1.0 \\\\\n",
            "row\\_0 &      &      \\\\\n",
            "\\midrule\n",
            "0     &   38 &   17 \\\\\n",
            "1     &    3 &    2 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/1791524234.py:44: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
            "  print(tableLL.to_latex(index=True))\n"
          ]
        }
      ],
      "source": [
        "logitL = None\n",
        "logitLasso = None\n",
        "logitL_opt = None\n",
        "logitL_opt_fit = None\n",
        "yChapL = None\n",
        "table = None\n",
        "\n",
        "# Grid of penalty parameters\n",
        "param = None\n",
        "param = [{\"C\": [0.001,0.01,0.1,0.5,0.75,0.9,1,1.5,2,2.5,5]}] # dictionary of the values of C used for GridSearchCV\n",
        "\n",
        "#COMMENT: We use the following codes to get logistic regression + l1 penalty (LASSO case)\n",
        "\n",
        "logitL = GridSearchCV(LogisticRegression(penalty=\"l1\", solver=\"liblinear\"), param ,cv =5, n_jobs = -1)\n",
        "logitLasso = logitL.fit(X_train, y_train)\n",
        "\n",
        "logitLasso.best_params_[\"C\"]\n",
        "\n",
        "# Computation of the loss and determining best parameter\n",
        "\n",
        "print(\"Best learning rate = %f, Best parameter  = %s\" %\n",
        "      (1.-logitLasso.best_score_ , logitLasso.best_params_))\n",
        "\n",
        "# Optimized model using the best parameter\n",
        "logitL_opt = LogisticRegression(C=0.001, penalty=\"l1\", solver=\"liblinear\")\n",
        "logitL_opt_fit = logitL_opt.fit(X_train, y_train)\n",
        "\n",
        "# custom threshold for binary predictions\n",
        "def custom_predictL(X, threshold):\n",
        "    probs = logitL_opt_fit.predict_proba(X)\n",
        "    return (probs[:, 1] > threshold).astype(int)\n",
        "\n",
        "# Prediction\n",
        "yChapL = custom_predictL(X_test, 0.37)\n",
        "\n",
        "tableLL = pd.crosstab(yChapL, y_test)\n",
        "print(tableLL.to_markdown())\n",
        "\n",
        "# Error on the test set\n",
        "print(\"Test accuracy - Logistic regression  Lasso = %f\" % (logitL_opt_fit.score(X_test, y_test)))\n",
        "r2_score_lasso = logitL_opt_fit.score(X_test, y_test)\n",
        "\n",
        "\n",
        "print(tableLL.to_latex(index=True))\n",
        "\n",
        "\n",
        "### Comment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSEbyuK79YGb"
      },
      "source": [
        "#### 2.5.2 Ridge optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQG_doe-9YGb",
        "outputId": "a6cedb30-e687-4a64-83c8-4b2acf6c415b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best score = 0.184043, Best parameter = {'C': 0.001}\n",
            "|   row_0 |   0.0 |   1.0 |\n",
            "|--------:|------:|------:|\n",
            "|       0 |    34 |    11 |\n",
            "|       1 |     7 |     8 |\n",
            "Test accuracy - Logistic regression Ridge = 0.683333\n",
            "\\begin{tabular}{lrr}\n",
            "\\toprule\n",
            "donation &  0.0 &  1.0 \\\\\n",
            "row\\_0 &      &      \\\\\n",
            "\\midrule\n",
            "0     &   34 &   11 \\\\\n",
            "1     &    7 &    8 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/935477074.py:42: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
            "  print(tableLR.to_latex(index=True))\n"
          ]
        }
      ],
      "source": [
        "logitR = None\n",
        "logitRidge = None\n",
        "logitR_opt = None\n",
        "logitR_opt_fit = None\n",
        "yChapR = None\n",
        "tableR = None\n",
        "\n",
        "# Grid of penalty parameters\n",
        "param = None\n",
        "param = [{\"C\":[0.001,0.005,0.01,0.1,0.25,0.5,0.75,1,2.5,5,7,10,15,20]}]\n",
        "\n",
        "\n",
        "logitR = GridSearchCV(LogisticRegression(penalty=\"l2\", solver=\"liblinear\"), param, cv = 5, n_jobs = -1)\n",
        "logitRidge = logitR.fit(X_train, y_train)\n",
        "logitRidge.best_params_[\"C\"]\n",
        "\n",
        "# Error computation\n",
        "print(\"Best score = %f, Best parameter = %s\" %\n",
        "      (1. - logitRidge.best_score_, logitRidge.best_params_))\n",
        "\n",
        "# Optimized model using the best parameter\n",
        "logitR_opt = LogisticRegression(C=15, penalty=\"l2\", solver=\"liblinear\")\n",
        "logitR_opt_fit = logitR_opt.fit(X_train, y_train)\n",
        "\n",
        "# custom threshold for binary predictions\n",
        "def custom_predictR(X, threshold):\n",
        "    probs = logitR_opt_fit.predict_proba(X)\n",
        "    return (probs[:, 1] > threshold).astype(int)\n",
        "\n",
        "# Prediction\n",
        "yChapR = custom_predictR(X_test, 0.249)\n",
        "\n",
        "# confusion matrix\n",
        "tableLR = pd.crosstab(yChapR, y_test)\n",
        "print(tableLR.to_markdown())\n",
        "\n",
        "# Error on the test set\n",
        "print(\"Test accuracy - Logistic regression Ridge = %f\" % (logitR_opt_fit.score(X_test, y_test)))\n",
        "r2_score_ridge = logitR_opt_fit.score(X_test, y_test)\n",
        "\n",
        "\n",
        "print(tableLR.to_latex(index=True))\n",
        "\n",
        "### Comment: slightly better than Lasso, in all runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO4VH2fB9YGc"
      },
      "source": [
        "## 3. Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9MSgmic9YGc",
        "outputId": "cc5106b6-b5b1-42e2-acf0-62422f2d7cf0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/4099974900.py:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_info['LLpred_donation'] = info_Lpred\n",
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/4099974900.py:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_info['LRpred_donation'] = info_Rpred\n",
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/4099974900.py:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_info['LPMpred_donation'] = info_LPMpred\n",
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/4099974900.py:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_info['Gpred_donation'] = info_Gpred\n",
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/4099974900.py:37: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_both['LLpred_donation_allwithnoinfo'] = both_Lpred\n",
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/4099974900.py:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_both['LRpred_donation_allwithnoinfo'] = both_Rpred\n",
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/4099974900.py:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_both['LPMpred_donation_allwithnoinfo'] = both_LPMpred\n",
            "/var/folders/hq/srpr_0qj2xn8xc6w5cyc_rhh0000gn/T/ipykernel_38054/4099974900.py:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_t46_both['Gpred_donation_allwithnoinfo'] = both_Gpred\n"
          ]
        }
      ],
      "source": [
        "# Predict Passive observer donations\n",
        "data_t46_info = None\n",
        "data_t46_info_sub = None\n",
        "data_m = None\n",
        "data_m0 = None\n",
        "\n",
        "data_t46_both = None\n",
        "data_t46_both_sub = None\n",
        "\n",
        "# subset with only no-info to predict on\n",
        "data_t46_info = data[((data[\"treatment\"]==4) | (data[\"treatment\"]==6)) & (data[\"info\"]==0)]\n",
        "data_t46_info_sub = data_t46_info[[\"age\", \"educ\", \"income\", \"female\", \"children\"]]\n",
        "data_t46_info_sub = data_t46_info_sub.fillna(data_t46_info_sub.mean()) # fill NAs with average\n",
        "\n",
        "# subset with both to predict on\n",
        "data_t46_both = data[(data[\"treatment\"]==4) | (data[\"treatment\"]==6)]\n",
        "data_t46_both_sub = data_t46_both[[\"age\", \"educ\", \"income\", \"female\", \"children\"]]\n",
        "data_t46_both_sub = data_t46_both_sub.fillna(data_t46_both_sub.mean()) # fill NAs with average\n",
        "\n",
        "# prediction\n",
        "info_Lpred = custom_predictL(data_t46_info_sub, 0.37)\n",
        "info_Rpred = custom_predictR(data_t46_info_sub, 0.249)\n",
        "info_LPMpred = custom_predictLPM(data_t46_info_sub, 0.2455)\n",
        "info_Gpred = custom_predictG(data_t46_info_sub, 0.2386)\n",
        "\n",
        "both_Lpred = custom_predictL(data_t46_both_sub, 0.37)\n",
        "both_Rpred = custom_predictR(data_t46_both_sub, 0.249)\n",
        "both_LPMpred = custom_predictLPM(data_t46_both_sub, 0.2455)\n",
        "both_Gpred = custom_predictG(data_t46_both_sub, 0.2386)\n",
        "\n",
        "\n",
        "data_t46_info['LLpred_donation'] = info_Lpred\n",
        "data_t46_info['LRpred_donation'] = info_Rpred\n",
        "data_t46_info['LPMpred_donation'] = info_LPMpred\n",
        "data_t46_info['Gpred_donation'] = info_Gpred\n",
        "\n",
        "data_t46_both['LLpred_donation_allwithnoinfo'] = both_Lpred\n",
        "data_t46_both['LRpred_donation_allwithnoinfo'] = both_Rpred\n",
        "data_t46_both['LPMpred_donation_allwithnoinfo'] = both_LPMpred\n",
        "data_t46_both['Gpred_donation_allwithnoinfo'] = both_Gpred\n",
        "\n",
        "# merging\n",
        "data_m0 = pd.merge(data_t46_both, data_t46_info[['ID', 'LLpred_donation', 'LRpred_donation', 'LPMpred_donation', 'Gpred_donation']], on='ID', how='left') # first merging no-info with all-subset\n",
        "data_m = pd.merge(data, data_m0[['ID', 'LLpred_donation', 'LRpred_donation', 'LPMpred_donation', 'Gpred_donation', 'LLpred_donation_allwithnoinfo', 'LRpred_donation_allwithnoinfo', 'LPMpred_donation_allwithnoinfo', 'Gpred_donation_allwithnoinfo']], on='ID', how='left') # second merging all-subset with whole data\n",
        "\n",
        "output_path2 = os.path.join(data_path, 'data_predicted_noinfo.dta')\n",
        "data_m.to_stata(output_path2, version=118)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "-R7CqIjFCuDP"
      },
      "outputs": [],
      "source": [
        "%reset -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "uRn0uLpt8wZY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "config_path = os.path.join('..', 'config.json')\n",
        "\n",
        "with open(config_path, 'r') as config_file:\n",
        "    config = json.load(config_file)\n",
        "\n",
        "\n",
        "data_path = config['data_path']\n",
        "\n",
        "input_path1 = os.path.join(data_path, 'data_predicted_info.dta')\n",
        "input_path2 = os.path.join(data_path, 'data_predicted_noinfo.dta')\n",
        "\n",
        "\n",
        "# merge no info and info prediction data sets\n",
        "data_info = None\n",
        "data_noinfo = None\n",
        "data_final = None\n",
        "data_final0 = None\n",
        "\n",
        "data_info = pd.read_stata(input_path1, convert_categoricals=False)\n",
        "data_noinfo = pd.read_stata(input_path2, convert_categoricals=False)\n",
        "\n",
        "data_final0 = data_info.fillna(data_noinfo)\n",
        "data_final = pd.merge(data_final0, data_noinfo[['ID', 'LLpred_donation_allwithnoinfo', 'LRpred_donation_allwithnoinfo', 'LPMpred_donation_allwithnoinfo', 'Gpred_donation_allwithnoinfo']], on='ID', how='left') # adding the four additional columns generated with no-info\n",
        "\n",
        "output_path1 = os.path.join(data_path, 'analysis.dta')\n",
        "\n",
        "data_final.to_stata(output_path1, version=118)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "lRx-gOjF7hqQ",
        "f7BKbZu1FMG7"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
